{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hourly Model Development\n",
    "\n",
    "Primary Evaluation Metric: AUROC\n",
    "Satisficing Metrics: F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Initialize ###################\n",
    "\n",
    "# Basics\n",
    "from pymongo import MongoClient\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import boto3\n",
    "import io\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "import spacy\n",
    "spacy.load('en')\n",
    "from nltk.corpus import stopwords\n",
    "import preprocessor as p\n",
    "\n",
    "# Model Infrastructure\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.pipeline import make_pipeline, Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier, XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Bring In Data #######################\n",
    "#Setup Mongo and create the database and collection\n",
    "User = os.environ['MONGODB_USER']\n",
    "password = os.environ['MONGODB_PASS']\n",
    "IP = os.environ['IP']\n",
    "\n",
    "client = MongoClient(IP, username=User, password=password)\n",
    "db = client['stock_tweets']\n",
    "\n",
    "#Grab references\n",
    "twitter_coll_reference = db.twitter\n",
    "iex_coll_reference = db.iex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 32.234617710113525 seconds ---\n"
     ]
    }
   ],
   "source": [
    "###################### Build Twitter Data Frames #####################\n",
    "\n",
    "start_time = time.time()\n",
    "# Create Data Frame\n",
    "twitter_data = pd.DataFrame(list(twitter_coll_reference.find()))\n",
    "\n",
    "# Need to convert the created_at to a time stamp and set to index\n",
    "twitter_data.index=pd.to_datetime(twitter_data['created_at'])\n",
    "\n",
    "# Delimited the Company List into separate rows\n",
    "delimited_twitter_data=[]\n",
    "\n",
    "for item in twitter_data.itertuples():\n",
    "    #twitter_dict={}\n",
    "    for company in item[1]:\n",
    "        twitter_dict={}\n",
    "        twitter_dict['created_at']=item[0]\n",
    "        twitter_dict['company']=company\n",
    "        twitter_dict['text']=item[11]\n",
    "        twitter_dict['user_followers_count']=item[12]\n",
    "        twitter_dict['user_name']=item[13]\n",
    "        twitter_dict['user_statuses_count']=item[15]\n",
    "        delimited_twitter_data.append(twitter_dict)\n",
    "\n",
    "delimited_twitter_df = pd.DataFrame(delimited_twitter_data) \n",
    "delimited_twitter_df.set_index('created_at', inplace=True)\n",
    "\n",
    "# Create hourly data frame\n",
    "twitter_delimited_hourly = delimited_twitter_df.groupby([pd.Grouper(freq=\"H\"), 'company']).count()['text'].to_frame()\n",
    "twitter_delimited_hourly.columns = ['Number_of_Tweets']\n",
    "\n",
    "# Concatenate the text with a space to not combine words.\n",
    "twitter_delimited_hourly['text']=delimited_twitter_df.groupby([pd.Grouper(freq=\"H\"), 'company'])['text'].apply(lambda x: ' '.join(x))\n",
    "# Number of Users\n",
    "twitter_delimited_hourly['Number_of_Users'] = delimited_twitter_df.groupby([pd.Grouper(freq=\"H\"), 'company'])['user_name'].nunique()\n",
    "\n",
    "# Rename Index\n",
    "twitter_delimited_hourly = twitter_delimited_hourly.reindex(twitter_delimited_hourly.index.rename(['Time', 'Company']))\n",
    "\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 14.78155779838562 seconds ---\n"
     ]
    }
   ],
   "source": [
    "##################### Build Stock Data Frames ###########################\n",
    "start_time = time.time()\n",
    "\n",
    "stock_data = pd.DataFrame(list(iex_coll_reference.find()))\n",
    "\n",
    "# Need to convert the created_at to a time stamp\n",
    "stock_data.index=pd.to_datetime(stock_data['latestUpdate'])\n",
    "stock_data['latestUpdate'] = pd.to_datetime(stock_data['latestUpdate'])\n",
    "#Group By hourly and stock price\n",
    "# Need to get the first stock price in teh hour, and then the last to take the difference to see how much change.\n",
    "stock_delimited_hourly = stock_data.sort_values('latestUpdate').groupby([pd.Grouper(freq=\"H\"), 'Ticker']).first()['latestPrice'].to_frame()\n",
    "stock_delimited_hourly.columns = ['First_Price']\n",
    "stock_delimited_hourly['Last_Price'] = stock_data.sort_values('latestUpdate').groupby([pd.Grouper(freq=\"H\"), 'Ticker']).last()['latestPrice']\n",
    "\n",
    "# Then need to take the difference and turn into a percentage.\n",
    "stock_delimited_hourly['Price_Percent_Change'] = ((stock_delimited_hourly['Last_Price'] \n",
    "                                                   - stock_delimited_hourly['First_Price'])/stock_delimited_hourly['First_Price'])*100\n",
    "\n",
    "# Need to also show Percent from open price\n",
    "stock_delimited_hourly['Open_Price'] = stock_data.groupby([pd.Grouper(freq=\"H\"), 'Ticker'])['open'].mean()\n",
    "stock_delimited_hourly['Price_Percent_Open'] = ((stock_delimited_hourly['Last_Price'] \n",
    "                                                 - stock_delimited_hourly['Open_Price'])/stock_delimited_hourly['Open_Price'])*100\n",
    "\n",
    "# Also include mean volume\n",
    "stock_delimited_hourly['Mean_Volume'] = stock_data.groupby([pd.Grouper(freq=\"H\"), 'Ticker'])['latestVolume'].mean()\n",
    "\n",
    "# Classification Labels\n",
    "stock_delimited_hourly['Price_Change'] = np.where(stock_delimited_hourly['Price_Percent_Change']>=0, 1, 0)\n",
    "stock_delimited_hourly['Open_Price_Change'] = np.where(stock_delimited_hourly['Price_Percent_Open']>=0, 1, 0)\n",
    "\n",
    "# Rename the Index\n",
    "stock_delimited_hourly = stock_delimited_hourly.reindex(stock_delimited_hourly.index.rename(['Time', 'Company']))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Company</th>\n",
       "      <th>Number_of_Tweets</th>\n",
       "      <th>text</th>\n",
       "      <th>Number_of_Users</th>\n",
       "      <th>First_Price</th>\n",
       "      <th>Last_Price</th>\n",
       "      <th>Price_Percent_Change</th>\n",
       "      <th>Open_Price</th>\n",
       "      <th>Price_Percent_Open</th>\n",
       "      <th>Mean_Volume</th>\n",
       "      <th>Price_Change</th>\n",
       "      <th>Open_Price_Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-12 18:00:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>94</td>\n",
       "      <td>@JoKiddo But how proprietary is that? Does it ...</td>\n",
       "      <td>83</td>\n",
       "      <td>181.730</td>\n",
       "      <td>181.690</td>\n",
       "      <td>-0.022011</td>\n",
       "      <td>180.23</td>\n",
       "      <td>0.810076</td>\n",
       "      <td>2.345693e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-03-12 18:00:00</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>103</td>\n",
       "      <td>Amazon hits $1600 $AMZN Americans reported one...</td>\n",
       "      <td>61</td>\n",
       "      <td>1600.745</td>\n",
       "      <td>1597.725</td>\n",
       "      <td>-0.188662</td>\n",
       "      <td>1592.60</td>\n",
       "      <td>0.321801</td>\n",
       "      <td>3.729830e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-03-12 18:00:00</td>\n",
       "      <td>BA</td>\n",
       "      <td>25</td>\n",
       "      <td>Thus, its cheaper for $AAPL to built than to b...</td>\n",
       "      <td>20</td>\n",
       "      <td>345.910</td>\n",
       "      <td>344.700</td>\n",
       "      <td>-0.349802</td>\n",
       "      <td>355.02</td>\n",
       "      <td>-2.906878</td>\n",
       "      <td>4.380395e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-03-12 18:00:00</td>\n",
       "      <td>BABA</td>\n",
       "      <td>10</td>\n",
       "      <td>Thus, its cheaper for $AAPL to built than to b...</td>\n",
       "      <td>8</td>\n",
       "      <td>192.900</td>\n",
       "      <td>192.930</td>\n",
       "      <td>0.015552</td>\n",
       "      <td>192.00</td>\n",
       "      <td>0.484375</td>\n",
       "      <td>1.405752e+07</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-03-12 18:00:00</td>\n",
       "      <td>BAC</td>\n",
       "      <td>5</td>\n",
       "      <td>Open an account with @RobinhoodApp and get a s...</td>\n",
       "      <td>3</td>\n",
       "      <td>32.980</td>\n",
       "      <td>32.870</td>\n",
       "      <td>-0.333535</td>\n",
       "      <td>32.67</td>\n",
       "      <td>0.612182</td>\n",
       "      <td>3.716710e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Time Company  Number_of_Tweets  \\\n",
       "0 2018-03-12 18:00:00    AAPL                94   \n",
       "1 2018-03-12 18:00:00    AMZN               103   \n",
       "2 2018-03-12 18:00:00      BA                25   \n",
       "3 2018-03-12 18:00:00    BABA                10   \n",
       "4 2018-03-12 18:00:00     BAC                 5   \n",
       "\n",
       "                                                text  Number_of_Users  \\\n",
       "0  @JoKiddo But how proprietary is that? Does it ...               83   \n",
       "1  Amazon hits $1600 $AMZN Americans reported one...               61   \n",
       "2  Thus, its cheaper for $AAPL to built than to b...               20   \n",
       "3  Thus, its cheaper for $AAPL to built than to b...                8   \n",
       "4  Open an account with @RobinhoodApp and get a s...                3   \n",
       "\n",
       "   First_Price  Last_Price  Price_Percent_Change  Open_Price  \\\n",
       "0      181.730     181.690             -0.022011      180.23   \n",
       "1     1600.745    1597.725             -0.188662     1592.60   \n",
       "2      345.910     344.700             -0.349802      355.02   \n",
       "3      192.900     192.930              0.015552      192.00   \n",
       "4       32.980      32.870             -0.333535       32.67   \n",
       "\n",
       "   Price_Percent_Open   Mean_Volume  Price_Change  Open_Price_Change  \n",
       "0            0.810076  2.345693e+07             0                  1  \n",
       "1            0.321801  3.729830e+06             0                  1  \n",
       "2           -2.906878  4.380395e+06             0                  0  \n",
       "3            0.484375  1.405752e+07             1                  1  \n",
       "4            0.612182  3.716710e+07             0                  1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################### Combine Data Frames ##############################\n",
    "hourly_df = pd.concat([twitter_delimited_hourly, stock_delimited_hourly], axis=1, join='inner')\n",
    "\n",
    "# To flatten after combined everything. \n",
    "hourly_df.reset_index(inplace=True)\n",
    "hourly_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 4.5234694480896 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Clean the Tweets\n",
    "p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.RESERVED, p.OPT.EMOJI, p.OPT.HASHTAG)\n",
    "def preprocess_tweet(tweet):\n",
    "    return p.clean(tweet)\n",
    "\n",
    "# Clean the tweets, by removing special characters\n",
    "start_time = time.time()\n",
    "hourly_df['Clean_text'] = hourly_df['text'].apply(lambda x: preprocess_tweet(x))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Company</th>\n",
       "      <th>Number_of_Tweets</th>\n",
       "      <th>text</th>\n",
       "      <th>Number_of_Users</th>\n",
       "      <th>First_Price</th>\n",
       "      <th>Last_Price</th>\n",
       "      <th>Price_Percent_Change</th>\n",
       "      <th>Open_Price</th>\n",
       "      <th>Price_Percent_Open</th>\n",
       "      <th>Mean_Volume</th>\n",
       "      <th>Price_Change</th>\n",
       "      <th>Open_Price_Change</th>\n",
       "      <th>Clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-12 18:00:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>94</td>\n",
       "      <td>@JoKiddo But how proprietary is that? Does it ...</td>\n",
       "      <td>83</td>\n",
       "      <td>181.730</td>\n",
       "      <td>181.690</td>\n",
       "      <td>-0.022011</td>\n",
       "      <td>180.23</td>\n",
       "      <td>0.810076</td>\n",
       "      <td>2.345693e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>But how proprietary is that? Does it really ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-03-12 18:00:00</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>103</td>\n",
       "      <td>Amazon hits $1600 $AMZN Americans reported one...</td>\n",
       "      <td>61</td>\n",
       "      <td>1600.745</td>\n",
       "      <td>1597.725</td>\n",
       "      <td>-0.188662</td>\n",
       "      <td>1592.60</td>\n",
       "      <td>0.321801</td>\n",
       "      <td>3.729830e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Amazon hits $1600 $AMZN Americans reported one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-03-12 18:00:00</td>\n",
       "      <td>BA</td>\n",
       "      <td>25</td>\n",
       "      <td>Thus, its cheaper for $AAPL to built than to b...</td>\n",
       "      <td>20</td>\n",
       "      <td>345.910</td>\n",
       "      <td>344.700</td>\n",
       "      <td>-0.349802</td>\n",
       "      <td>355.02</td>\n",
       "      <td>-2.906878</td>\n",
       "      <td>4.380395e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Thus, its cheaper for $AAPL to built than to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-03-12 18:00:00</td>\n",
       "      <td>BABA</td>\n",
       "      <td>10</td>\n",
       "      <td>Thus, its cheaper for $AAPL to built than to b...</td>\n",
       "      <td>8</td>\n",
       "      <td>192.900</td>\n",
       "      <td>192.930</td>\n",
       "      <td>0.015552</td>\n",
       "      <td>192.00</td>\n",
       "      <td>0.484375</td>\n",
       "      <td>1.405752e+07</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Thus, its cheaper for $AAPL to built than to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-03-12 18:00:00</td>\n",
       "      <td>BAC</td>\n",
       "      <td>5</td>\n",
       "      <td>Open an account with @RobinhoodApp and get a s...</td>\n",
       "      <td>3</td>\n",
       "      <td>32.980</td>\n",
       "      <td>32.870</td>\n",
       "      <td>-0.333535</td>\n",
       "      <td>32.67</td>\n",
       "      <td>0.612182</td>\n",
       "      <td>3.716710e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Open an account with and get a stock like $HPQ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Time Company  Number_of_Tweets  \\\n",
       "0 2018-03-12 18:00:00    AAPL                94   \n",
       "1 2018-03-12 18:00:00    AMZN               103   \n",
       "2 2018-03-12 18:00:00      BA                25   \n",
       "3 2018-03-12 18:00:00    BABA                10   \n",
       "4 2018-03-12 18:00:00     BAC                 5   \n",
       "\n",
       "                                                text  Number_of_Users  \\\n",
       "0  @JoKiddo But how proprietary is that? Does it ...               83   \n",
       "1  Amazon hits $1600 $AMZN Americans reported one...               61   \n",
       "2  Thus, its cheaper for $AAPL to built than to b...               20   \n",
       "3  Thus, its cheaper for $AAPL to built than to b...                8   \n",
       "4  Open an account with @RobinhoodApp and get a s...                3   \n",
       "\n",
       "   First_Price  Last_Price  Price_Percent_Change  Open_Price  \\\n",
       "0      181.730     181.690             -0.022011      180.23   \n",
       "1     1600.745    1597.725             -0.188662     1592.60   \n",
       "2      345.910     344.700             -0.349802      355.02   \n",
       "3      192.900     192.930              0.015552      192.00   \n",
       "4       32.980      32.870             -0.333535       32.67   \n",
       "\n",
       "   Price_Percent_Open   Mean_Volume  Price_Change  Open_Price_Change  \\\n",
       "0            0.810076  2.345693e+07             0                  1   \n",
       "1            0.321801  3.729830e+06             0                  1   \n",
       "2           -2.906878  4.380395e+06             0                  0   \n",
       "3            0.484375  1.405752e+07             1                  1   \n",
       "4            0.612182  3.716710e+07             0                  1   \n",
       "\n",
       "                                          Clean_text  \n",
       "0  But how proprietary is that? Does it really ma...  \n",
       "1  Amazon hits $1600 $AMZN Americans reported one...  \n",
       "2  Thus, its cheaper for $AAPL to built than to b...  \n",
       "3  Thus, its cheaper for $AAPL to built than to b...  \n",
       "4  Open an account with and get a stock like $HPQ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Between Outcome and Features\n",
    "features = hourly_df[['Number_of_Tweets', 'Number_of_Users','Mean_Volume','Clean_text']]\n",
    "classification_price = hourly_df['Price_Change']\n",
    "classification_open = hourly_df['Open_Price_Change']\n",
    "regression_price = hourly_df['Price_Percent_Change']\n",
    "regression_open = hourly_df['Price_Percent_Open']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Price Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3766\n",
       "1    3690\n",
       "Name: Price_Change, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check is Data is imbalanced\n",
    "hourly_df['Price_Change'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Data to avoid Leakage\n",
    "#splitting into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,classification_price,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 44.77558612823486 seconds ---\n"
     ]
    }
   ],
   "source": [
    "####### Old Way ########\n",
    "# Was separating the data transformation from the mode because I used to do it this way when there is a lot of data. Since there is not, can do it\n",
    "# in one step\n",
    "start_time = time.time()\n",
    "# Create lemmatizer using spacy\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "\n",
    "def custom_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens if not token.is_punct])\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, stop_words='english',\n",
    "                             lowercase=True, use_idf=True, max_df=2, max_features=1000,\n",
    "                             min_df=2, norm='l2', smooth_idf=True, ngram_range=(1, 2))\n",
    "\n",
    "\n",
    "train_tfidf = vectorizer.fit(X_train['Clean_text'])\n",
    "tweets_train = train_tfidf.transform(X_train['Clean_text'])\n",
    "tweets_test = train_tfidf.transform(X_test['Clean_text'])\n",
    "\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.03      0.53      0.07        51\n",
      "          1       0.97      0.48      0.64      1441\n",
      "\n",
      "avg / total       0.93      0.48      0.62      1492\n",
      "\n",
      "0.4839142091152815\n",
      "-- Execution time: 0.24537062644958496 seconds ---\n"
     ]
    }
   ],
   "source": [
    "################ Logistic Regression ##########################\n",
    "start_time = time.time()\n",
    "parameters = {\n",
    "                'penalty':['l1','l2'],\n",
    "                'C':[0.1, 0.001, 1, 10, 100],\n",
    "                'class_weight':['balanced']\n",
    "               \n",
    "              }\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "grid = GridSearchCV(lr, parameters, scoring='accuracy', cv=3, verbose=0)\n",
    "#Fit the Data\n",
    "grid.fit(tweets_train, y_train)\n",
    "y = grid.predict(tweets_test)\n",
    "print(classification_report(y, y_test))\n",
    "print(grid.score(tweets_test, y_test))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'class_weight': 'balanced', 'penalty': 'l1'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### XGB #############################\n",
    "start_time = time.time()\n",
    "xgb_model = XGBClassifier(learning_rate =0.1, n_estimators=140, max_depth=5, min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', scale_pos_weight=1, seed=27) \n",
    " \n",
    "\n",
    "parameters = {'n_jobs':[-1],\n",
    "             'max_depth':range(3,10,2),\n",
    "             'min_child_weight':range(1,6,2)}\n",
    "\n",
    "clf = GridSearchCV(xgb_model, parameters,cv=3, verbose=0,n_jobs=1)\n",
    "clf.fit(tweets_train, y_train)\n",
    "print(clf.score(tweets_test, y_test))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# New Way ##################\n",
    "# Will create pipeline that combines all the steps and then apply the model at the end.\n",
    "# Since all the features are apart of the features dataframe, for the NLP only need the clean text, but still want to add other things.\n",
    "\n",
    "# Will create a class to handle this. \n",
    "class DataSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, features):\n",
    "        if self.key=='text':\n",
    "            return features['Clean_text']\n",
    "        else:\n",
    "            return features[['Number_of_Tweets', 'Number_of_Users','Mean_Volume']]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 95.41935133934021 seconds ---\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.32      0.53      0.40       478\n",
      "          1       0.68      0.49      0.57      1014\n",
      "\n",
      "avg / total       0.57      0.50      0.51      1492\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####### Logistic Regression ############\n",
    "\n",
    "start_time = time.time()\n",
    "# Create lemmatizer using spacy\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "\n",
    "# Define Model\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "# Define custom Tokenizer\n",
    "def custom_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens if not token.is_punct])\n",
    "\n",
    "# Define Vectorizer\n",
    "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, stop_words='english',\n",
    "                             lowercase=True, use_idf=True, max_df=2, max_features=1000,\n",
    "                             min_df=2, norm='l2', smooth_idf=True, ngram_range=(1, 2))\n",
    "\n",
    "# Define Pipeline and Feature Union\n",
    "pipeline = Pipeline([\n",
    "    # Use Feature Union to combine features from the Tweet and other features gathered\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for text\n",
    "            ('tweet', Pipeline([\n",
    "                ('selector', DataSelector(key='text')),\n",
    "                ('vectidf', vectorizer)\n",
    "                                \n",
    "            ])),\n",
    "            \n",
    "            # Pipeline for getting other features\n",
    "            ('other', Pipeline([\n",
    "                ('seclector', DataSelector(key='other'))\n",
    "             ])),\n",
    "        ],\n",
    "                       \n",
    "    )),\n",
    "    # Use Logistic Regression Classifier\n",
    "    ('lr', lr_model)\n",
    "])\n",
    "\n",
    "# Grid Search\n",
    "parameters = {\n",
    "                'lr__penalty':['l1'],\n",
    "                'lr__C':[10],\n",
    "                'lr__class_weight':['balanced'],\n",
    "                'union__transformer_weights':[{'tweet':0.5, 'other':0.5},{'tweet':0.2, 'other':0.8},{'tweet':0.8, 'other':0.2}]\n",
    "               \n",
    "              }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, scoring='accuracy', cv=3, verbose=0, n_jobs=5)\n",
    "\n",
    "# Fit the grid\n",
    "grid.fit(X_train, y_train)\n",
    "# Predictions\n",
    "y = grid.predict(X_test)\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))\n",
    "print(classification_report(y, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr__C': 10,\n",
       " 'lr__class_weight': 'balanced',\n",
       " 'lr__penalty': 'l1',\n",
       " 'union__transformer_weights': {'other': 0.8, 'tweet': 0.2}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.505700871898055"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3807\n",
       "1    3649\n",
       "Name: Open_Price_Change, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check is Data is imbalanced\n",
    "hourly_df['Open_Price_Change'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Data to avoid Leakage\n",
    "#splitting into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,classification_open,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 86.12768578529358 seconds ---\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.36      0.53      0.43       512\n",
      "          1       0.68      0.52      0.59       980\n",
      "\n",
      "avg / total       0.57      0.52      0.53      1492\n",
      "\n",
      "0.5454491547352078\n"
     ]
    }
   ],
   "source": [
    "####### Logistic Regression ############\n",
    "\n",
    "start_time = time.time()\n",
    "# Create lemmatizer using spacy\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "\n",
    "# Define Model\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "\n",
    "# Define custom Tokenizer\n",
    "def custom_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens if not token.is_punct])\n",
    "\n",
    "\n",
    "# Define Vectorizer\n",
    "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, stop_words='english',\n",
    "                             lowercase=True, use_idf=True, max_df=2, max_features=1000,\n",
    "                             min_df=2, norm='l2', smooth_idf=True, ngram_range=(1, 2))\n",
    "\n",
    "# Define Pipeline and Feature Union\n",
    "pipeline = Pipeline([\n",
    "    # Use Feature Union to combine features from the Tweet and other features gathered\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for text\n",
    "            ('tweet', Pipeline([\n",
    "                ('selector', DataSelector(key='text')),\n",
    "                ('vectidf', vectorizer),\n",
    "                #('svd', TruncatedSVD(500)),\n",
    "                #('norm',Normalizer(copy=False))\n",
    "                                \n",
    "            ])),\n",
    "            \n",
    "            # Pipeline for getting other features\n",
    "            ('other', Pipeline([\n",
    "                ('seclector', DataSelector(key='other'))\n",
    "             ])),\n",
    "            \n",
    "        ],\n",
    "                       \n",
    "    )),\n",
    "    # Use Logistic Regression Classifier\n",
    "    ('lr', lr_model)\n",
    "])\n",
    "\n",
    "\n",
    "# Grid Search\n",
    "parameters = {\n",
    "                'lr__penalty':['l1'],\n",
    "                'lr__C':[10],\n",
    "                'lr__class_weight':['balanced'],\n",
    "                'union__transformer_weights':[{'tweet':0.5, 'other':0.5},{'tweet':0.2, 'other':0.8},{'tweet':0.8, 'other':0.2}]\n",
    "               \n",
    "              }\n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, scoring='roc_auc', cv=2, verbose=0, n_jobs=5)\n",
    "\n",
    "# Fit the grid\n",
    "grid.fit(X_train, y_train)\n",
    "# Predictions\n",
    "y = grid.predict(X_test)\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))\n",
    "print(classification_report(y, y_test))\n",
    "\n",
    "prediction_proba = grid.predict_proba(X_test)\n",
    "prediction_proba = [p[1] for p in prediction_proba]\n",
    "print(roc_auc_score(y_test, prediction_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5661916236066759"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 203.15611171722412 seconds ---\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.49      0.49       762\n",
      "          1       0.48      0.49      0.48       730\n",
      "\n",
      "avg / total       0.49      0.49      0.49      1492\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####### XGB ############\n",
    "warnings.filterwarnings('ignore')\n",
    "start_time = time.time()\n",
    "# Create lemmatizer using spacy\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "\n",
    "# Define Model\n",
    "xgb_model = XGBClassifier(learning_rate =0.1, n_estimators=140, max_depth=5, min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', scale_pos_weight=1, seed=27) \n",
    " \n",
    "\n",
    "\n",
    "# Define custom Tokenizer\n",
    "def custom_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens if not token.is_punct])\n",
    "\n",
    "# Define Vectorizer\n",
    "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, stop_words='english',\n",
    "                             lowercase=True, use_idf=True, max_df=2, max_features=1000,\n",
    "                             min_df=2, norm='l2', smooth_idf=True, ngram_range=(1, 2))\n",
    "\n",
    "# Define Pipeline and Feature Union\n",
    "pipeline = Pipeline([\n",
    "    # Use Feature Union to combine features from the Tweet and other features gathered\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for text\n",
    "            ('tweet', Pipeline([\n",
    "                ('selector', DataSelector(key='text')),\n",
    "                ('vectidf', vectorizer)\n",
    "                                \n",
    "            ])),\n",
    "            \n",
    "            # Pipeline for getting other features\n",
    "            ('other', Pipeline([\n",
    "                ('seclector', DataSelector(key='other'))\n",
    "             ])),\n",
    "        ],\n",
    "                       \n",
    "    )),\n",
    "    # Use Logistic Regression Classifier\n",
    "    ('xgb', xgb_model)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Grid Search\n",
    "parameters = {\n",
    "             'xgb__n_jobs':[-1],\n",
    "             #'xgb__max_depth':range(3,10,2),\n",
    "             #'xgb__min_child_weight':range(1,6,2),\n",
    "             'union__transformer_weights':[{'tweet':0.5, 'other':0.5},{'tweet':0.2, 'other':0.8},{'tweet':0.8, 'other':0.2}]\n",
    "               \n",
    "              }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, scoring='accuracy', cv=2, verbose=0, n_jobs=1)\n",
    "\n",
    "# Fit the grid\n",
    "grid.fit(X_train, y_train)\n",
    "# Predictions\n",
    "y = grid.predict(X_test)\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))\n",
    "print(classification_report(y, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
