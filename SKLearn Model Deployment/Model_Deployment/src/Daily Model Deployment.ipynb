{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Model Deployment\n",
    "\n",
    "1. Load Model from S3\n",
    "2. Query past 10 mintues of data from MongoDB\n",
    "3. Build Features\n",
    "4. Make Predictions\n",
    "5. Log predictions to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Initialize ###################\n",
    "\n",
    "# Basics\n",
    "from pymongo import MongoClient\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import boto3\n",
    "import io\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "from datetime import date, datetime, timedelta\n",
    "import subprocess\n",
    "\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "import spacy\n",
    "spacy.load('en')\n",
    "from nltk.corpus import stopwords\n",
    "import preprocessor as p\n",
    "\n",
    "# Model Infrastructure\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.pipeline import make_pipeline, Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn import metrics\n",
    "import dill as pickle\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Database Setup\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import inspect\n",
    "from sqlalchemy import MetaData\n",
    "from sqlalchemy import Table\n",
    "from sqlalchemy import Column\n",
    "from sqlalchemy import Integer, String, DateTime, Float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily_Model_predictions Table Exists\n"
     ]
    }
   ],
   "source": [
    "########## Database Setup #################\n",
    "User = os.environ['DB_USER']\n",
    "password = os.environ['DB_PWD']\n",
    "dbname = os.environ['DB_NAME']\n",
    "IP = os.environ['IP']\n",
    "\n",
    "engine = create_engine('mysql+mysqlconnector://{}:{}@{}:3306/{}'.format(User,\n",
    "                                                                        password, IP, dbname), echo=False)\n",
    "conn = engine.connect()\n",
    "\n",
    "# Check to see if the tables are created and if not, create them\n",
    "meta = MetaData(engine)\n",
    "\n",
    "# Create prediction table\n",
    "if not engine.dialect.has_table(engine, 'daily_model_predictions'):\n",
    "    print('Daily_Model_predictions Table does not exist')\n",
    "    print('Daily_Model_predictions Table being created....')\n",
    "    # Time, Source, Current Count, Count Diff\n",
    "    t1 = Table('daily_model_predictions', meta,\n",
    "               Column('run_time', DateTime, default=datetime.utcnow),\n",
    "               Column('model_name', String(30)),\n",
    "               Column('model_version_number', Integer),\n",
    "               Column('Company', String(30)),\n",
    "               Column('Prediction', Integer))\n",
    "    t1.create()\n",
    "else:\n",
    "    print('Daily_Model_predictions Table Exists')\n",
    "\n",
    "# Create table object\n",
    "meta = MetaData(engine, reflect=True)\n",
    "daily_model_predictions_table = meta.tables['daily_model_predictions']    \n",
    "\n",
    "# Write Function\n",
    "def database_log(name, version_number, company, prediction):\n",
    "    #Need to log these items to a database.\n",
    "        \n",
    "    ins = daily_model_predictions_table.insert().values(\n",
    "            run_time = datetime.now(),\n",
    "            model_name = name,\n",
    "            model_version_number = version_number,\n",
    "            Company = company,\n",
    "            Prediction = prediction\n",
    "               )\n",
    "    conn.execute(ins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['aws', 's3', 'cp', 's3://brandyn-twitter-sentiment-analysis/Models/Daily_Stock_Prediction_latest.pk', './Models'], returncode=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the Model\n",
    "subprocess.run(['aws', 's3','cp','s3://brandyn-twitter-sentiment-analysis/Models/Daily_Stock_Prediction_latest.pk','./Models'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "### Validate Pickle ###\n",
    "filename = 'Daily_Stock_Prediction_latest.pk'\n",
    "\n",
    "with open('./Models/'+filename, 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Bring In Data #######################\n",
    "#Setup Mongo and create the database and collection\n",
    "User = os.environ['MONGODB_USER']\n",
    "password = os.environ['MONGODB_PASS']\n",
    "IP = os.environ['IP']\n",
    "\n",
    "client = MongoClient(IP, username=User, password=password)\n",
    "db = client['stock_tweets']\n",
    "\n",
    "#Grab references\n",
    "twitter_coll_reference = db.twitter\n",
    "iex_coll_reference = db.iex\n",
    "\n",
    "# Create Time bound\n",
    "time_bound =  pd.to_datetime(datetime.utcnow() - timedelta(hours = 72))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Get Model Version Number #############\n",
    "query = '''\n",
    "        SELECT model_version_number\n",
    "        FROM model_scores\n",
    "        ORDER BY run_time DESC\n",
    "        LIMIT 1\n",
    "        '''\n",
    "version_number = conn.execute(query).fetchone()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Setup Redis #####################\n",
    "# Connect to Redis-DataStore\n",
    "REDIS = redis.Redis(host=IP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Define Functions #####################\n",
    "\n",
    "# Build Twitter Data Frames \n",
    "\n",
    "def get_twitter_data():\n",
    "    # Create Data Frame from Mongo DB\n",
    "    twitter_data = pd.DataFrame(list(twitter_coll_reference.find()))\n",
    "\n",
    "    # Take a subset of the data, dont need all points to convert and this greatly speeds up\n",
    "    twitter_data_subset = twitter_data.tail(2000)\n",
    "\n",
    "    # Need to convert the created_at to a time stamp and set to index\n",
    "    twitter_data_subset['created_at'] = pd.to_datetime(twitter_data_subset['created_at'])\n",
    "    twitter_data_subset.index=twitter_data_subset['created_at']\n",
    "\n",
    "    # Create time bounded dataframe\n",
    "    twitter_data = twitter_data_subset[twitter_data_subset['created_at'] >= time_bound]\n",
    "\n",
    "    # Delimited the Company List into separate rows\n",
    "    delimited_twitter_data=[]\n",
    "\n",
    "    for item in twitter_data.itertuples():\n",
    "        #twitter_dict={}\n",
    "        for company in item[1]:\n",
    "            twitter_dict={}\n",
    "            twitter_dict['created_at']=item[0]\n",
    "            twitter_dict['company']=company\n",
    "            twitter_dict['text']=item[11]\n",
    "            twitter_dict['user_followers_count']=item[12]\n",
    "            twitter_dict['user_name']=item[13]\n",
    "            twitter_dict['user_statuses_count']=item[15]\n",
    "            delimited_twitter_data.append(twitter_dict)\n",
    "\n",
    "    delimited_twitter_df = pd.DataFrame(delimited_twitter_data) \n",
    "    delimited_twitter_df.set_index('created_at', inplace=True)\n",
    "\n",
    "    # Create hourly data frame\n",
    "    twitter_delimited_daily = delimited_twitter_df.groupby([pd.Grouper(freq=\"D\"), 'company']).count()['text'].to_frame()\n",
    "    twitter_delimited_daily.columns = ['Number_of_Tweets']\n",
    "\n",
    "    # Concatenate the text with a space to not combine words.\n",
    "    twitter_delimited_daily['text']=delimited_twitter_df.groupby([pd.Grouper(freq=\"D\"), 'company'])['text'].apply(lambda x: ' '.join(x))\n",
    "    # Number of Users\n",
    "    twitter_delimited_daily['Number_of_Users'] = delimited_twitter_df.groupby([pd.Grouper(freq=\"D\"), 'company'])['user_name'].nunique()\n",
    "\n",
    "    # Rename Index\n",
    "    twitter_delimited_daily = twitter_delimited_daily.reindex(twitter_delimited_daily.index.rename(['Time', 'Company']))\n",
    "    return twitter_delimited_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Stock Data Frames \n",
    "\n",
    "def get_stock_data():\n",
    "    stock_data = pd.DataFrame(list(iex_coll_reference.find()))\n",
    "    \n",
    "    # Take a subset of the data, dont need all points to convert and this greatly speeds up\n",
    "    stock_data_subset = stock_data.tail(2000)\n",
    "    \n",
    "    # Need to convert the created_at to a time stamp\n",
    "    stock_data_subset['latestUpdate'] = pd.to_datetime(stock_data_subset['latestUpdate'])\n",
    "    stock_data_subset.index=stock_data_subset['latestUpdate']\n",
    "    \n",
    "    # Create time bounded dataframe\n",
    "    stock_data = stock_data_subset[stock_data_subset['latestUpdate'] >= time_bound]\n",
    "    \n",
    "    # Create delimited dataframe\n",
    "    stock_delimited_daily = stock_data.groupby([pd.Grouper(freq=\"D\"), 'Ticker'])['latestVolume'].mean().to_frame()\n",
    "    stock_delimited_daily.columns = ['Mean_Volume']\n",
    "    \n",
    "    # Rename the Index\n",
    "    stock_delimited_daily = stock_delimited_daily.reindex(stock_delimited_daily.index.rename(['Time', 'Company']))\n",
    "    return stock_delimited_daily\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the Tweets\n",
    "p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.RESERVED, p.OPT.EMOJI, p.OPT.HASHTAG)\n",
    "def preprocess_tweet(tweet):\n",
    "    return p.clean(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction function\n",
    "# Create lemmatizer using spacy\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "\n",
    "def make_predictions():\n",
    "    # Only pull stock info if the DataFlag is set.\n",
    "    if int(REDIS.get('Data_On')) == 1:\n",
    "    \n",
    "        # Call Functions\n",
    "        twitter_delimited_daily = get_twitter_data()\n",
    "        stock_delimited_daily = get_stock_data()\n",
    "\n",
    "        # Combine Dataframes\n",
    "        daily_df = pd.concat([twitter_delimited_daily, stock_delimited_daily], axis=1, join='inner')\n",
    "\n",
    "        # To flatten after combined everything. \n",
    "        daily_df.reset_index(inplace=True)\n",
    "\n",
    "        # Clean the tweets, by removing special characters\n",
    "        daily_df['Clean_text'] = daily_df['text'].apply(lambda x: preprocess_tweet(x))\n",
    "\n",
    "        # Split Between Outcome and Features\n",
    "        features = daily_df[['Number_of_Tweets', 'Number_of_Users','Mean_Volume','Clean_text']]\n",
    "\n",
    "        # Predictions\n",
    "        y = model.predict(features)\n",
    "\n",
    "        # Set Predictions to daily dataframe\n",
    "        daily_df['predictions'] = y\n",
    "\n",
    "        # Log to Database\n",
    "        for item in daily_df.itertuples():\n",
    "            version_number = int(version_number)\n",
    "            company = item[2]\n",
    "            prediction = item[8]\n",
    "            database_log(filename, version_number, company, prediction)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### Execute #######################################\n",
    "\n",
    "# Setup Schedule\n",
    "schedule.clear()\n",
    "schedule.every(20).minutes.do(make_predictions)\n",
    "\n",
    "# Execute\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
