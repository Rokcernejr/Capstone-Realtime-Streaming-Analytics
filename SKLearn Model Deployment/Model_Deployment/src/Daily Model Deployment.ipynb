{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Model Deployment\n",
    "\n",
    "1. Load Model from S3\n",
    "2. Query past 10 mintues of data from MongoDB\n",
    "3. Build Features\n",
    "4. Make Predictions\n",
    "5. Log predictions to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Initialize ###################\n",
    "\n",
    "# Basics\n",
    "from pymongo import MongoClient\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import boto3\n",
    "import io\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "from datetime import date, datetime, timedelta\n",
    "import subprocess\n",
    "\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "import spacy\n",
    "spacy.load('en')\n",
    "from nltk.corpus import stopwords\n",
    "import preprocessor as p\n",
    "\n",
    "# Model Infrastructure\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.pipeline import make_pipeline, Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn import metrics\n",
    "import dill as pickle\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Database Setup\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import inspect\n",
    "from sqlalchemy import MetaData\n",
    "from sqlalchemy import Table\n",
    "from sqlalchemy import Column\n",
    "from sqlalchemy import Integer, String, DateTime, Float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_predictions Table Exists\n"
     ]
    }
   ],
   "source": [
    "########## Database Setup #################\n",
    "User = os.environ['DB_USER']\n",
    "password = os.environ['DB_PWD']\n",
    "dbname = os.environ['DB_NAME']\n",
    "IP = os.environ['IP']\n",
    "\n",
    "engine = create_engine('mysql+mysqlconnector://{}:{}@{}:3306/{}'.format(User,\n",
    "                                                                        password, IP, dbname), echo=False)\n",
    "conn = engine.connect()\n",
    "\n",
    "# Check to see if the tables are created and if not, create them\n",
    "meta = MetaData(engine)\n",
    "\n",
    "# Create prediction table\n",
    "if not engine.dialect.has_table(engine, 'daily_model_predictions'):\n",
    "    print('Daily_Model_predictions Table does not exist')\n",
    "    print('Daily_Model_predictions Table being created....')\n",
    "    # Time, Source, Current Count, Count Diff\n",
    "    t1 = Table('daily_model_predictions', meta,\n",
    "               Column('run_time', DateTime, default=datetime.utcnow),\n",
    "               Column('model_name', String(30)),\n",
    "               Column('model_version_number', Integer),\n",
    "               Column('Company', String(30)),\n",
    "               Column('Prediction', Integer))\n",
    "    t1.create()\n",
    "else:\n",
    "    print('Daily_Model_predictions Table Exists')\n",
    "\n",
    "# Create table object\n",
    "meta = MetaData(engine, reflect=True)\n",
    "daily_model_predictions_table = meta.tables['daily_model_predictions']    \n",
    "\n",
    "# Write Function\n",
    "def database_log(name, version_number, company, prediction):\n",
    "    #Need to log these items to a database.\n",
    "        \n",
    "    ins = daily_model_predictions_table.insert().values(\n",
    "            run_time = datetime.now(),\n",
    "            model_name = name,\n",
    "            model_version_number = version_number,\n",
    "            Company = company,\n",
    "            Prediction = prediction\n",
    "               )\n",
    "    conn.execute(ins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['aws', 's3', 'cp', 's3://brandyn-twitter-sentiment-analysis/Models/Daily_Stock_Prediction_latest.pk', './Models'], returncode=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the Model\n",
    "subprocess.run(['aws', 's3','cp','s3://brandyn-twitter-sentiment-analysis/Models/Daily_Stock_Prediction_latest.pk','./Models'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "### Validate Pickle ###\n",
    "filename = 'Daily_Stock_Prediction_latest.pk'\n",
    "\n",
    "with open('./Models/'+filename, 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Bring In Data #######################\n",
    "#Setup Mongo and create the database and collection\n",
    "User = os.environ['MONGODB_USER']\n",
    "password = os.environ['MONGODB_PASS']\n",
    "IP = os.environ['IP']\n",
    "\n",
    "client = MongoClient(IP, username=User, password=password)\n",
    "db = client['stock_tweets']\n",
    "\n",
    "#Grab references\n",
    "twitter_coll_reference = db.twitter\n",
    "iex_coll_reference = db.iex\n",
    "\n",
    "# Create Time bound\n",
    "ten_min_bound =  pd.to_datetime(datetime.utcnow() - timedelta(hours = 48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Build Twitter Data Frames #####################\n",
    "\n",
    "def get_twitter_data():\n",
    "    # Create Data Frame from Mongo DB\n",
    "    twitter_data = pd.DataFrame(list(twitter_coll_reference.find()))\n",
    "\n",
    "    # Take a subset of the data, dont need all points to convert and this greatly speeds up\n",
    "    twitter_data_subset = twitter_data.tail(400)\n",
    "\n",
    "    # Need to convert the created_at to a time stamp and set to index\n",
    "    twitter_data_subset['created_at'] = pd.to_datetime(twitter_data_subset['created_at'])\n",
    "    twitter_data_subset.index=twitter_data_subset['created_at']\n",
    "\n",
    "    # Create time bounded dataframe\n",
    "    twitter_data = twitter_data_subset[twitter_data_subset['created_at'] >= ten_min_bound]\n",
    "\n",
    "    # Delimited the Company List into separate rows\n",
    "    delimited_twitter_data=[]\n",
    "\n",
    "    for item in twitter_data.itertuples():\n",
    "        #twitter_dict={}\n",
    "        for company in item[1]:\n",
    "            twitter_dict={}\n",
    "            twitter_dict['created_at']=item[0]\n",
    "            twitter_dict['company']=company\n",
    "            twitter_dict['text']=item[11]\n",
    "            twitter_dict['user_followers_count']=item[12]\n",
    "            twitter_dict['user_name']=item[13]\n",
    "            twitter_dict['user_statuses_count']=item[15]\n",
    "            delimited_twitter_data.append(twitter_dict)\n",
    "\n",
    "    delimited_twitter_df = pd.DataFrame(delimited_twitter_data) \n",
    "    delimited_twitter_df.set_index('created_at', inplace=True)\n",
    "\n",
    "    # Create hourly data frame\n",
    "    twitter_delimited_daily = delimited_twitter_df.groupby([pd.Grouper(freq=\"D\"), 'company']).count()['text'].to_frame()\n",
    "    twitter_delimited_daily.columns = ['Number_of_Tweets']\n",
    "\n",
    "    # Concatenate the text with a space to not combine words.\n",
    "    twitter_delimited_daily['text']=delimited_twitter_df.groupby([pd.Grouper(freq=\"D\"), 'company'])['text'].apply(lambda x: ' '.join(x))\n",
    "    # Number of Users\n",
    "    twitter_delimited_daily['Number_of_Users'] = delimited_twitter_df.groupby([pd.Grouper(freq=\"D\"), 'company'])['user_name'].nunique()\n",
    "\n",
    "    # Rename Index\n",
    "    twitter_delimited_daily = twitter_delimited_daily.reindex(twitter_delimited_daily.index.rename(['Time', 'Company']))\n",
    "    return twitter_delimited_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Build Stock Data Frames ###########################\n",
    "\n",
    "def get_stock_data():\n",
    "    stock_data = pd.DataFrame(list(iex_coll_reference.find()))\n",
    "    \n",
    "    # Take a subset of the data, dont need all points to convert and this greatly speeds up\n",
    "    stock_data_subset = stock_data.tail(400)\n",
    "    \n",
    "    # Need to convert the created_at to a time stamp\n",
    "    stock_data_subset['latestUpdate'] = pd.to_datetime(stock_data_subset['latestUpdate'])\n",
    "    stock_data_subset.index=stock_data_subset['latestUpdate']\n",
    "    \n",
    "    # Create time bounded dataframe\n",
    "    stock_data = stock_data_subset[stock_data_subset['latestUpdate'] >= ten_min_bound]\n",
    "    \n",
    "    # Create delimited dataframe\n",
    "    stock_delimited_daily = stock_data.groupby([pd.Grouper(freq=\"D\"), 'Ticker'])['latestVolume'].mean().to_frame()\n",
    "    stock_delimited_daily.columns = ['Mean_Volume']\n",
    "    \n",
    "    # Rename the Index\n",
    "    stock_delimited_daily = stock_delimited_daily.reindex(stock_delimited_daily.index.rename(['Time', 'Company']))\n",
    "    return stock_delimited_daily\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Company</th>\n",
       "      <th>Number_of_Tweets</th>\n",
       "      <th>text</th>\n",
       "      <th>Number_of_Users</th>\n",
       "      <th>Mean_Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-05-18</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>31</td>\n",
       "      <td>Step by Step Dividend Investing: A Beginner's ...</td>\n",
       "      <td>25</td>\n",
       "      <td>1.366410e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-05-18</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>41</td>\n",
       "      <td>Helluvah specific price there.  Four decimal p...</td>\n",
       "      <td>37</td>\n",
       "      <td>1.285560e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-05-18</td>\n",
       "      <td>BA</td>\n",
       "      <td>52</td>\n",
       "      <td>5.18.18 Elliott Wave Updates For Momentum: $BA...</td>\n",
       "      <td>43</td>\n",
       "      <td>3.043739e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-05-18</td>\n",
       "      <td>BABA</td>\n",
       "      <td>12</td>\n",
       "      <td>5.18.18 Elliott Wave Updates For Momentum: $BA...</td>\n",
       "      <td>12</td>\n",
       "      <td>7.864113e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-05-18</td>\n",
       "      <td>BAC</td>\n",
       "      <td>10</td>\n",
       "      <td>#DivestFromWar\\n\\nDivest from\\nBank of America...</td>\n",
       "      <td>9</td>\n",
       "      <td>4.648342e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Time Company  Number_of_Tweets  \\\n",
       "0 2018-05-18    AAPL                31   \n",
       "1 2018-05-18    AMZN                41   \n",
       "2 2018-05-18      BA                52   \n",
       "3 2018-05-18    BABA                12   \n",
       "4 2018-05-18     BAC                10   \n",
       "\n",
       "                                                text  Number_of_Users  \\\n",
       "0  Step by Step Dividend Investing: A Beginner's ...               25   \n",
       "1  Helluvah specific price there.  Four decimal p...               37   \n",
       "2  5.18.18 Elliott Wave Updates For Momentum: $BA...               43   \n",
       "3  5.18.18 Elliott Wave Updates For Momentum: $BA...               12   \n",
       "4  #DivestFromWar\\n\\nDivest from\\nBank of America...                9   \n",
       "\n",
       "    Mean_Volume  \n",
       "0  1.366410e+07  \n",
       "1  1.285560e+06  \n",
       "2  3.043739e+06  \n",
       "3  7.864113e+06  \n",
       "4  4.648342e+07  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call Functions\n",
    "twitter_delimited_daily = get_twitter_data()\n",
    "stock_delimited_daily = get_stock_data()\n",
    "\n",
    "# Combine Dataframes\n",
    "daily_df = pd.concat([twitter_delimited_daily, stock_delimited_daily], axis=1, join='inner')\n",
    "\n",
    "# To flatten after combined everything. \n",
    "daily_df.reset_index(inplace=True)\n",
    "daily_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the Tweets\n",
    "p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.RESERVED, p.OPT.EMOJI, p.OPT.HASHTAG)\n",
    "def preprocess_tweet(tweet):\n",
    "    return p.clean(tweet)\n",
    "\n",
    "# Clean the tweets, by removing special characters\n",
    "start_time = time.time()\n",
    "daily_df['Clean_text'] = daily_df['text'].apply(lambda x: preprocess_tweet(x))\n",
    "\n",
    "# Split Between Outcome and Features\n",
    "features = daily_df[['Number_of_Tweets', 'Number_of_Users','Mean_Volume','Clean_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y = model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
