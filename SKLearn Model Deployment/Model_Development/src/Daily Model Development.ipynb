{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Model Development\n",
    "\n",
    "1. Build Model in Sklearn\n",
    "2. Serialize and put in S3\n",
    "3. Log information to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Initialize ###################\n",
    "\n",
    "# Basics\n",
    "from pymongo import MongoClient\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import boto3\n",
    "import io\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "from datetime import date, datetime\n",
    "\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "import spacy\n",
    "spacy.load('en')\n",
    "from nltk.corpus import stopwords\n",
    "import preprocessor as p\n",
    "\n",
    "# Model Infrastructure\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.pipeline import make_pipeline, Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn import metrics\n",
    "import dill as pickle\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Database Setup\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import inspect\n",
    "from sqlalchemy import MetaData\n",
    "from sqlalchemy import Table\n",
    "from sqlalchemy import Column\n",
    "from sqlalchemy import Integer, String, DateTime, Float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_scores Table Exists\n"
     ]
    }
   ],
   "source": [
    "####################################### Database Setup ###################################\n",
    "User = os.environ['DB_USER']\n",
    "password = os.environ['DB_PWD']\n",
    "dbname = os.environ['DB_NAME']\n",
    "IP = os.environ['IP']\n",
    "\n",
    "engine = create_engine('mysql+mysqlconnector://{}:{}@{}:3306/{}'.format(User,\n",
    "                                                                        password, IP, dbname), echo=False)\n",
    "conn = engine.connect()\n",
    "\n",
    "# Check to see if the tables are created and if not, create them\n",
    "meta = MetaData(engine)\n",
    "\n",
    "# Create log table\n",
    "if not engine.dialect.has_table(engine, 'model_scores'):\n",
    "    print('Model_Scores Table does not exist')\n",
    "    print('Model_Scores Table being created....')\n",
    "    # Time, Source, Current Count, Count Diff\n",
    "    t1 = Table('model_scores', meta,\n",
    "               Column('run_time', DateTime, default=datetime.utcnow),\n",
    "               Column('model_name', String(30)),\n",
    "               Column('model_version_number', Integer),\n",
    "               Column('auc_score', Float),\n",
    "               Column('build_time_sec', Float))\n",
    "    t1.create()\n",
    "else:\n",
    "    print('Model_scores Table Exists')\n",
    "\n",
    "# Create table object\n",
    "meta = MetaData(engine, reflect=True)\n",
    "model_scores_table = meta.tables['model_scores']    \n",
    "\n",
    "# Write Function\n",
    "def database_log(name, version_number, auc, build_time):\n",
    "    #Need to log these items to a database.\n",
    "        \n",
    "    ins = model_scores_table.insert().values(\n",
    "            run_time = datetime.now(),\n",
    "            model_name = name,\n",
    "            model_version_number = version_number,\n",
    "            auc_score = auc,\n",
    "            build_time_sec = build_time\n",
    "               )\n",
    "    conn.execute(ins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Bring In Data #######################\n",
    "#Setup Mongo and create the database and collection\n",
    "User = os.environ['MONGODB_USER']\n",
    "password = os.environ['MONGODB_PASS']\n",
    "IP = os.environ['IP']\n",
    "\n",
    "client = MongoClient(IP, username=User, password=password)\n",
    "db = client['stock_tweets']\n",
    "\n",
    "#Grab references\n",
    "twitter_coll_reference = db.twitter\n",
    "iex_coll_reference = db.iex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 35.07836937904358 seconds ---\n"
     ]
    }
   ],
   "source": [
    "###################### Build Twitter Data Frames #####################\n",
    "\n",
    "start_time = time.time()\n",
    "# Create Data Frame\n",
    "twitter_data = pd.DataFrame(list(twitter_coll_reference.find()))\n",
    "\n",
    "# Need to convert the created_at to a time stamp and set to index\n",
    "twitter_data.index=pd.to_datetime(twitter_data['created_at'])\n",
    "\n",
    "# Delimited the Company List into separate rows\n",
    "delimited_twitter_data=[]\n",
    "\n",
    "for item in twitter_data.itertuples():\n",
    "    #twitter_dict={}\n",
    "    for company in item[1]:\n",
    "        twitter_dict={}\n",
    "        twitter_dict['created_at']=item[0]\n",
    "        twitter_dict['company']=company\n",
    "        twitter_dict['text']=item[11]\n",
    "        twitter_dict['user_followers_count']=item[12]\n",
    "        twitter_dict['user_name']=item[13]\n",
    "        twitter_dict['user_statuses_count']=item[15]\n",
    "        delimited_twitter_data.append(twitter_dict)\n",
    "\n",
    "delimited_twitter_df = pd.DataFrame(delimited_twitter_data) \n",
    "delimited_twitter_df.set_index('created_at', inplace=True)\n",
    "\n",
    "# Create hourly data frame\n",
    "twitter_delimited_daily = delimited_twitter_df.groupby([pd.Grouper(freq=\"D\"), 'company']).count()['text'].to_frame()\n",
    "twitter_delimited_daily.columns = ['Number_of_Tweets']\n",
    "\n",
    "# Concatenate the text with a space to not combine words.\n",
    "twitter_delimited_daily['text']=delimited_twitter_df.groupby([pd.Grouper(freq=\"D\"), 'company'])['text'].apply(lambda x: ' '.join(x))\n",
    "# Number of Users\n",
    "twitter_delimited_daily['Number_of_Users'] = delimited_twitter_df.groupby([pd.Grouper(freq=\"D\"), 'company'])['user_name'].nunique()\n",
    "\n",
    "# Rename Index\n",
    "twitter_delimited_daily = twitter_delimited_daily.reindex(twitter_delimited_daily.index.rename(['Time', 'Company']))\n",
    "\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 12.368745803833008 seconds ---\n"
     ]
    }
   ],
   "source": [
    "##################### Build Stock Data Frames ###########################\n",
    "start_time = time.time()\n",
    "\n",
    "stock_data = pd.DataFrame(list(iex_coll_reference.find()))\n",
    "\n",
    "# Need to convert the created_at to a time stamp\n",
    "stock_data.index=pd.to_datetime(stock_data['latestUpdate'])\n",
    "stock_data['latestUpdate'] = pd.to_datetime(stock_data['latestUpdate'])\n",
    "#Group By hourly and stock price\n",
    "# Need to get the first stock price in teh hour, and then the last to take the difference to see how much change.\n",
    "stock_delimited_daily = stock_data.sort_values('latestUpdate').groupby([pd.Grouper(freq=\"D\"), 'Ticker']).first()['latestPrice'].to_frame()\n",
    "stock_delimited_daily.columns = ['First_Price']\n",
    "stock_delimited_daily['Last_Price'] = stock_data.sort_values('latestUpdate').groupby([pd.Grouper(freq=\"D\"), 'Ticker']).last()['latestPrice']\n",
    "\n",
    "# Then need to take the difference and turn into a percentage.\n",
    "stock_delimited_daily['Price_Percent_Change'] = ((stock_delimited_daily['Last_Price'] \n",
    "                                                   - stock_delimited_daily['First_Price'])/stock_delimited_daily['First_Price'])*100\n",
    "\n",
    "# Need to also show Percent from open price\n",
    "stock_delimited_daily['Open_Price'] = stock_data.groupby([pd.Grouper(freq=\"D\"), 'Ticker'])['open'].mean()\n",
    "stock_delimited_daily['Price_Percent_Open'] = ((stock_delimited_daily['Last_Price'] \n",
    "                                                 - stock_delimited_daily['Open_Price'])/stock_delimited_daily['Open_Price'])*100\n",
    "\n",
    "# Also include mean volume\n",
    "stock_delimited_daily['Mean_Volume'] = stock_data.groupby([pd.Grouper(freq=\"D\"), 'Ticker'])['latestVolume'].mean()\n",
    "\n",
    "# Classification Labels\n",
    "stock_delimited_daily['Price_Change'] = np.where(stock_delimited_daily['Price_Percent_Change']>=0, 1, 0)\n",
    "stock_delimited_daily['Open_Price_Change'] = np.where(stock_delimited_daily['Price_Percent_Open']>=0, 1, 0)\n",
    "\n",
    "# Rename the Index\n",
    "stock_delimited_daily = stock_delimited_daily.reindex(stock_delimited_daily.index.rename(['Time', 'Company']))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Number_of_Tweets</th>\n",
       "      <th>text</th>\n",
       "      <th>Number_of_Users</th>\n",
       "      <th>First_Price</th>\n",
       "      <th>Last_Price</th>\n",
       "      <th>Price_Percent_Change</th>\n",
       "      <th>Open_Price</th>\n",
       "      <th>Price_Percent_Open</th>\n",
       "      <th>Mean_Volume</th>\n",
       "      <th>Price_Change</th>\n",
       "      <th>Open_Price_Change</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <th>Company</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2018-03-12</th>\n",
       "      <th>AAPL</th>\n",
       "      <td>403</td>\n",
       "      <td>@JoKiddo But how proprietary is that? Does it ...</td>\n",
       "      <td>258</td>\n",
       "      <td>181.730</td>\n",
       "      <td>181.75</td>\n",
       "      <td>0.011005</td>\n",
       "      <td>180.23</td>\n",
       "      <td>0.843367</td>\n",
       "      <td>2.767373e+07</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <td>275</td>\n",
       "      <td>Amazon hits $1600 $AMZN Americans reported one...</td>\n",
       "      <td>162</td>\n",
       "      <td>1600.745</td>\n",
       "      <td>1598.39</td>\n",
       "      <td>-0.147119</td>\n",
       "      <td>1592.60</td>\n",
       "      <td>0.363556</td>\n",
       "      <td>4.376277e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BA</th>\n",
       "      <td>137</td>\n",
       "      <td>Thus, its cheaper for $AAPL to built than to b...</td>\n",
       "      <td>94</td>\n",
       "      <td>345.910</td>\n",
       "      <td>344.19</td>\n",
       "      <td>-0.497239</td>\n",
       "      <td>355.02</td>\n",
       "      <td>-3.050532</td>\n",
       "      <td>5.150044e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BABA</th>\n",
       "      <td>50</td>\n",
       "      <td>Thus, its cheaper for $AAPL to built than to b...</td>\n",
       "      <td>37</td>\n",
       "      <td>192.900</td>\n",
       "      <td>192.74</td>\n",
       "      <td>-0.082945</td>\n",
       "      <td>192.00</td>\n",
       "      <td>0.385417</td>\n",
       "      <td>1.622245e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAC</th>\n",
       "      <td>51</td>\n",
       "      <td>Open an account with @RobinhoodApp and get a s...</td>\n",
       "      <td>35</td>\n",
       "      <td>32.980</td>\n",
       "      <td>32.84</td>\n",
       "      <td>-0.424500</td>\n",
       "      <td>32.67</td>\n",
       "      <td>0.520355</td>\n",
       "      <td>4.670738e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Number_of_Tweets  \\\n",
       "Time       Company                     \n",
       "2018-03-12 AAPL                  403   \n",
       "           AMZN                  275   \n",
       "           BA                    137   \n",
       "           BABA                   50   \n",
       "           BAC                    51   \n",
       "\n",
       "                                                                 text  \\\n",
       "Time       Company                                                      \n",
       "2018-03-12 AAPL     @JoKiddo But how proprietary is that? Does it ...   \n",
       "           AMZN     Amazon hits $1600 $AMZN Americans reported one...   \n",
       "           BA       Thus, its cheaper for $AAPL to built than to b...   \n",
       "           BABA     Thus, its cheaper for $AAPL to built than to b...   \n",
       "           BAC      Open an account with @RobinhoodApp and get a s...   \n",
       "\n",
       "                    Number_of_Users  First_Price  Last_Price  \\\n",
       "Time       Company                                             \n",
       "2018-03-12 AAPL                 258      181.730      181.75   \n",
       "           AMZN                 162     1600.745     1598.39   \n",
       "           BA                    94      345.910      344.19   \n",
       "           BABA                  37      192.900      192.74   \n",
       "           BAC                   35       32.980       32.84   \n",
       "\n",
       "                    Price_Percent_Change  Open_Price  Price_Percent_Open  \\\n",
       "Time       Company                                                         \n",
       "2018-03-12 AAPL                 0.011005      180.23            0.843367   \n",
       "           AMZN                -0.147119     1592.60            0.363556   \n",
       "           BA                  -0.497239      355.02           -3.050532   \n",
       "           BABA                -0.082945      192.00            0.385417   \n",
       "           BAC                 -0.424500       32.67            0.520355   \n",
       "\n",
       "                     Mean_Volume  Price_Change  Open_Price_Change  \n",
       "Time       Company                                                 \n",
       "2018-03-12 AAPL     2.767373e+07             1                  1  \n",
       "           AMZN     4.376277e+06             0                  1  \n",
       "           BA       5.150044e+06             0                  0  \n",
       "           BABA     1.622245e+07             0                  1  \n",
       "           BAC      4.670738e+07             0                  1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################### Combine Data Frames ##############################\n",
    "daily_df = pd.concat([twitter_delimited_daily, stock_delimited_daily], axis=1, join='inner')\n",
    "daily_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To flatten after combined everything. \n",
    "daily_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 4.789851427078247 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Clean the Tweets\n",
    "p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.RESERVED, p.OPT.EMOJI, p.OPT.HASHTAG)\n",
    "def preprocess_tweet(tweet):\n",
    "    return p.clean(tweet)\n",
    "\n",
    "# Clean the tweets, by removing special characters\n",
    "start_time = time.time()\n",
    "daily_df['Clean_text'] = daily_df['text'].apply(lambda x: preprocess_tweet(x))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Between Outcome and Features\n",
    "features = daily_df[['Company','Number_of_Tweets', 'Number_of_Users','Mean_Volume','Clean_text']]\n",
    "classification_price = daily_df['Price_Change']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to leverage the Company name so need to create dummy variables. \n",
    "cat_feats = ['Company']\n",
    "features = pd.get_dummies(features, columns=cat_feats, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Selector class to handle feature union \n",
    "class DataSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, features):\n",
    "        if self.key=='text':\n",
    "            return features['Clean_text']\n",
    "        else:\n",
    "            return features.loc[:, features.columns != 'Clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Data to avoid Leakage\n",
    "#splitting into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,classification_price,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7310216121846969\n",
      "-- Execution time: 64.08627223968506 seconds ---\n"
     ]
    }
   ],
   "source": [
    "####### Build Pipeline ###########\n",
    "start_time = time.time()\n",
    "# Create lemmatizer using spacy\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "\n",
    "# Define Model\n",
    "lr_model = LogisticRegression(n_jobs=5, penalty='l2', class_weight='balanced', solver='newton-cg', C=10)\n",
    "\n",
    "# Define custom Tokenizer\n",
    "def custom_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens if not token.is_punct])\n",
    "\n",
    "# Define Vectorizer\n",
    "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, stop_words='english',\n",
    "                             lowercase=True, use_idf=True, max_df=2,\n",
    "                             min_df=2, norm='l2', smooth_idf=True, ngram_range=(1, 2))\n",
    "\n",
    "# Define Pipeline and Feature Union\n",
    "pipeline = Pipeline([\n",
    "    # Use Feature Union to combine features from the Tweet and other features gathered\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for text\n",
    "            ('tweet', Pipeline([\n",
    "                ('selector', DataSelector(key='text')),\n",
    "                ('vectidf', vectorizer),\n",
    "                ('svd', TruncatedSVD(1000)),\n",
    "                ('norm',Normalizer(copy=False))\n",
    "                                \n",
    "            ])),\n",
    "            \n",
    "            # Pipeline for getting other features\n",
    "            ('other', Pipeline([\n",
    "                ('seclector', DataSelector(key='other'))\n",
    "             ])),\n",
    "        ],\n",
    "        # weight components in FeatureUnion\n",
    "        transformer_weights={'tweet':0.8, 'other':0.2},\n",
    "        \n",
    "    )),\n",
    "    # Use Logistic Regression Classifier\n",
    "    ('lr', lr_model)\n",
    "])\n",
    "\n",
    "\n",
    "# Fit the grid\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "# AUROC Score\n",
    "prediction_proba = pipeline.predict_proba(X_test)\n",
    "prediction_proba = [p[1] for p in prediction_proba]\n",
    "auc = roc_auc_score(y_test, prediction_proba)\n",
    "print(auc)\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 74.49164509773254 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#### Train final model on the full dataset #####\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create lemmatizer using spacy\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "\n",
    "# Define Model\n",
    "lr_model = LogisticRegression(n_jobs=5, penalty='l2', class_weight='balanced', solver='newton-cg', C=10)\n",
    "\n",
    "# Define custom Tokenizer\n",
    "def custom_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens if not token.is_punct])\n",
    "\n",
    "# Define Vectorizer\n",
    "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, stop_words='english',\n",
    "                             lowercase=True, use_idf=True, max_df=2,\n",
    "                             min_df=2, norm='l2', smooth_idf=True, ngram_range=(1, 2))\n",
    "\n",
    "# Define Pipeline and Feature Union\n",
    "model = Pipeline([\n",
    "    # Use Feature Union to combine features from the Tweet and other features gathered\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for text\n",
    "            ('tweet', Pipeline([\n",
    "                ('selector', DataSelector(key='text')),\n",
    "                ('vectidf', vectorizer),\n",
    "                ('svd', TruncatedSVD(1000)),\n",
    "                ('norm',Normalizer(copy=False))\n",
    "                                \n",
    "            ])),\n",
    "            \n",
    "            # Pipeline for getting other features\n",
    "            ('other', Pipeline([\n",
    "                ('seclector', DataSelector(key='other'))\n",
    "             ])),\n",
    "        ],\n",
    "        # weight components in FeatureUnion\n",
    "        transformer_weights={'tweet':0.8, 'other':0.2},\n",
    "        \n",
    "    )),\n",
    "    # Use Logistic Regression Classifier\n",
    "    ('lr', lr_model)\n",
    "])\n",
    "\n",
    "\n",
    "# Fit the grid\n",
    "model.fit(features,classification_price)\n",
    "\n",
    "build_time = time.time() - start_time\n",
    "print(\"-- Execution time: %s seconds ---\" % (build_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log to Database\n",
    "\n",
    "version = str(int(time.time()))\n",
    "\n",
    "database_log('Daily_Stock_Prediction', version, float(auc), float(build_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Serialize the model ############\n",
    "\n",
    "\n",
    "# Create Model Name\n",
    "name = 'Daily_Stock_Prediction_'+ version + '.pk'\n",
    "latest = 'Daily_Stock_Prediction_latest.pk'\n",
    "\n",
    "with open('./Models/'+name, 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "with open('./Models/'+latest, 'wb') as file:\n",
    "    pickle.dump(model, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Upload to S3 #########################\n",
    "\n",
    "AWS_ACCESS_KEY_ID = os.environ['AWS_ACCESS_KEY_ID']\n",
    "AWS_ACCESS_KEY_SECRET = os.environ['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "\n",
    "def upload_files(path):\n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n",
    "        aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'],\n",
    "        \n",
    "    )\n",
    "    s3 = session.resource('s3')\n",
    "    bucket = s3.Bucket('brandyn-twitter-sentiment-analysis')\n",
    " \n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(subdir, file)\n",
    "            with open(full_path, 'rb') as data:\n",
    "                bucket.put_object(Key=full_path, Body=data)\n",
    " \n",
    "upload_files('Models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Validate Pickle ###\n",
    "filename = 'Daily_Stock_Prediction_latest.pk'\n",
    "\n",
    "with open('./Models/'+filename, 'rb') as f:\n",
    "    model_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99674914213473\n"
     ]
    }
   ],
   "source": [
    "# AUROC Score - Score is high because the model includes the test data. \n",
    "prediction_proba = model_test.predict_proba(X_test)\n",
    "prediction_proba = [p[1] for p in prediction_proba]\n",
    "auc = roc_auc_score(y_test, prediction_proba)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
